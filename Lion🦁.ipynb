{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "1MwCGWVVO5iMtBUKPPhZqp31gVRi3vHFr",
      "authorship_tag": "ABX9TyPIb+Y+WTulnIyfqqS70FTm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cxw1219/Lion/blob/main/Lion%F0%9F%A6%81.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4mcxgfMDphI",
        "outputId": "19e110a7-9442-4ecb-ec0f-ce7821059a8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Setup and imports\n",
        "!pip install torch pandas numpy scikit-learn matplotlib seaborn\n",
        "import torch, pandas as pd, numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loading functions\n",
        "def load_data(file_path):\n",
        "    return pd.read_csv(file_path, parse_dates=['timestamp'])\n",
        "\n",
        "def calculate_usdx(forex_data):\n",
        "    weights = {\n",
        "        'EUR': -0.576, 'JPY': 0.136, 'GBP': -0.119,\n",
        "        'CAD': 0.091, 'SEK': 0.042, 'CHF': 0.036\n",
        "    }\n",
        "    usdx = 50.14348112\n",
        "    for curr, weight in weights.items():\n",
        "        if curr == 'EUR':\n",
        "            pair = forex_data['EUR_USD']\n",
        "        elif curr == 'GBP':\n",
        "            pair = forex_data['GBP_USD']\n",
        "        else:\n",
        "            pair = forex_data[f'USD_{curr}']\n",
        "        usdx *= pair['close'] ** weight\n",
        "    return pd.Series(usdx, index=pair.index)"
      ],
      "metadata": {
        "id": "gfdyJegFD55P"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature engineering\n",
        "def create_features(df):\n",
        "    return pd.DataFrame({\n",
        "        'close': df['close'],\n",
        "        'returns': df['close'].pct_change(fill_method=None),\n",
        "        'volatility': df['close'].pct_change(fill_method=None).rolling(20).std()\n",
        "    })"
      ],
      "metadata": {
        "id": "g5gX53daD503"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GoldPredictor(torch.nn.Module):\n",
        "    def __init__(self, input_dim=9):\n",
        "        super().__init__()\n",
        "        self.embed = torch.nn.Linear(input_dim, 8)  # Project to dimension divisible by 2\n",
        "        encoder_layer = torch.nn.TransformerEncoderLayer(\n",
        "            d_model=8,  # Now divisible by 2\n",
        "            nhead=2,\n",
        "            dim_feedforward=256,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=3)\n",
        "        self.decoder = torch.nn.Linear(8, 12)\n",
        "        self.softplus = torch.nn.Softplus()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(0) if x.dim() == 2 else x\n",
        "        x = self.embed(x)\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x[:, -1, :])\n",
        "        mean, scale = x.chunk(2, dim=-1)\n",
        "        return mean, self.softplus(scale)"
      ],
      "metadata": {
        "id": "KZ0SpRMZD5wj"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data processing pipeline with chunking\n",
        "def prepare_data(sequence_length=144, chunk_size=1000):\n",
        "    data_path = '/content/drive/MyDrive/UpsharesDownshares/ONN/Data'\n",
        "\n",
        "    def load_in_chunks(filepath):\n",
        "        chunks = pd.read_csv(filepath, dtype={'timestamp': 'object'}, chunksize=chunk_size)\n",
        "        df = pd.concat(chunks)\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%d %H:%M:%S%z')\n",
        "        return df.set_index('timestamp').sort_index()\n",
        "\n",
        "    # Load gold data\n",
        "    gold_data = create_features(load_in_chunks(f'{data_path}/XAU_USD.csv'))\n",
        "\n",
        "    # Load bond data\n",
        "    bond_data = create_features(load_in_chunks(f'{data_path}/USB02Y_USD.csv'))\n",
        "\n",
        "    # Process forex data\n",
        "    forex_files = ['EUR_USD.csv', 'USD_JPY.csv', 'GBP_USD.csv',\n",
        "                   'USD_CAD.csv', 'USD_SEK.csv', 'USD_CHF.csv']\n",
        "\n",
        "    forex_data = {}\n",
        "    for f in forex_files:\n",
        "        forex_data[f.split('.')[0]] = load_in_chunks(f'{data_path}/{f}')\n",
        "\n",
        "    usdx = create_features(pd.DataFrame({'close': calculate_usdx(forex_data)}))\n",
        "\n",
        "    # Combine and align on timestamps\n",
        "    data = pd.concat([gold_data, usdx, bond_data], axis=1).dropna()\n",
        "\n",
        "    # Split and scale\n",
        "    train_mask = data.index <= '2020-12'\n",
        "    scaler = StandardScaler().fit(data[train_mask])\n",
        "    scaled_data = scaler.transform(data)\n",
        "\n",
        "    return scaled_data, scaler"
      ],
      "metadata": {
        "id": "q1DQ6hCvD5sP"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunked sequence training\n",
        "def train_model(model, train_data, epochs=100, seq_length=144, chunk_size=1000):\n",
        "    torch.cuda.empty_cache()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    # Create sequences\n",
        "    n_sequences = len(train_data) - seq_length - 6\n",
        "    n_chunks = n_sequences // chunk_size + 1\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    patience = 5\n",
        "    no_improve = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for chunk in range(n_chunks):\n",
        "            start_idx = chunk * chunk_size\n",
        "            end_idx = min(start_idx + chunk_size, n_sequences)\n",
        "\n",
        "            sequences = torch.from_numpy(np.array([\n",
        "                train_data[i:i+seq_length]\n",
        "                for i in range(start_idx, end_idx)\n",
        "            ])).float().to(device)\n",
        "\n",
        "            targets = torch.from_numpy(np.array([\n",
        "                train_data[i+seq_length:i+seq_length+6, 0]\n",
        "                for i in range(start_idx, end_idx)\n",
        "            ])).float().to(device)\n",
        "\n",
        "            model.train()\n",
        "            mean, std = model(sequences)\n",
        "            loss = -torch.distributions.Normal(mean, std).log_prob(targets).mean()\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Free memory\n",
        "            del sequences\n",
        "            del targets\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            print(f'\\rEpoch {epoch+1}/{epochs} Chunk {chunk+1}/{n_chunks} Loss: {loss:.4f} GPU: {torch.cuda.memory_allocated()/1e9:.1f}GB', end='')\n",
        "\n",
        "        avg_loss = total_loss / n_chunks\n",
        "        print(f'\\nEpoch {epoch+1} Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "        if abs(avg_loss - best_loss) < 0.0001:\n",
        "            no_improve += 1\n",
        "            if no_improve >= patience:\n",
        "                print(\"Early stopping - loss converged\")\n",
        "                break\n",
        "        else:\n",
        "            no_improve = 0\n",
        "            if avg_loss < best_loss:\n",
        "                best_loss = avg_loss"
      ],
      "metadata": {
        "id": "LHlavP1UD5nu"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference functions\n",
        "def predict(model, recent_data, scaler):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        mean, std = model(recent_data)\n",
        "        mean_cpu = mean.cpu().numpy()\n",
        "        std_cpu = std.cpu().numpy()\n",
        "\n",
        "        # Only unscale the gold price (first feature)\n",
        "        preds = mean_cpu * scaler.scale_[0] + scaler.mean_[0]\n",
        "        conf = std_cpu * scaler.scale_[0]\n",
        "\n",
        "        return preds, conf\n",
        "\n",
        "def display_predictions(predictions, confidence):\n",
        "    # Create timestamps for predictions only (not including current time)\n",
        "    times = pd.date_range(start=pd.Timestamp.now(), periods=6, freq='10min')\n",
        "    return pd.DataFrame({\n",
        "        'Price': predictions,\n",
        "        'Lower_CI': predictions - 1.96 * confidence,\n",
        "        'Upper_CI': predictions + 1.96 * confidence\n",
        "    }, index=times)"
      ],
      "metadata": {
        "id": "bHtJCiKkD5jy"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting pipeline...\")\n",
        "\n",
        "    print(\"Checking GPU...\", end=' ')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using {device}\")\n",
        "\n",
        "    print(\"Loading data...\", end=' ')\n",
        "    data, scaler = prepare_data()\n",
        "    print(f\"Loaded {len(data)} samples\")\n",
        "\n",
        "    print(\"Splitting data...\", end=' ')\n",
        "    split_idx = int(len(data) * 0.8)\n",
        "    train_data = data[:split_idx]\n",
        "    test_data = data[split_idx:]\n",
        "    print(f\"Train: {len(train_data)}, Test: {len(test_data)}\")\n",
        "\n",
        "    print(\"Initializing model...\", end=' ')\n",
        "    model = GoldPredictor().to(device)\n",
        "    print(\"Done\")\n",
        "\n",
        "    print(\"\\nStarting training...\")\n",
        "    train_model(model, train_data)\n",
        "\n",
        "    print(\"\\nGenerating predictions...\")\n",
        "    latest_data = torch.FloatTensor(data[-144:]).unsqueeze(0).to(device)\n",
        "    preds, conf = predict(model, latest_data, scaler)\n",
        "    print(\"\\nPredictions:\")\n",
        "    print(display_predictions(preds[0], conf[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "USuzFt_ND5fI",
        "outputId": "0167696b-e918-4618-9661-7314ca845703"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting pipeline...\n",
            "Checking GPU... Using cuda\n",
            "Loading data... Loaded 320011 samples\n",
            "Splitting data... Train: 256008, Test: 64003\n",
            "Initializing model... Done\n",
            "\n",
            "Starting training...\n",
            "Epoch 1/100 Chunk 256/256 Loss: 3.8705 GPU: 0.0GB\n",
            "Epoch 1 Average Loss: 2.7753\n",
            "Epoch 2/100 Chunk 256/256 Loss: 3.3993 GPU: 0.0GB\n",
            "Epoch 2 Average Loss: 2.2609\n",
            "Epoch 3/100 Chunk 256/256 Loss: 3.0607 GPU: 0.0GB\n",
            "Epoch 3 Average Loss: 2.1199\n",
            "Epoch 4/100 Chunk 256/256 Loss: 2.8096 GPU: 0.0GB\n",
            "Epoch 4 Average Loss: 2.0084\n",
            "Epoch 5/100 Chunk 256/256 Loss: 2.5952 GPU: 0.0GB\n",
            "Epoch 5 Average Loss: 1.8694\n",
            "Epoch 6/100 Chunk 256/256 Loss: 2.4051 GPU: 0.0GB\n",
            "Epoch 6 Average Loss: 1.6924\n",
            "Epoch 7/100 Chunk 256/256 Loss: 2.2521 GPU: 0.0GB\n",
            "Epoch 7 Average Loss: 1.5037\n",
            "Epoch 8/100 Chunk 256/256 Loss: 2.1278 GPU: 0.0GB\n",
            "Epoch 8 Average Loss: 1.3991\n",
            "Epoch 9/100 Chunk 256/256 Loss: 2.0273 GPU: 0.0GB\n",
            "Epoch 9 Average Loss: 1.3309\n",
            "Epoch 10/100 Chunk 256/256 Loss: 1.9454 GPU: 0.0GB\n",
            "Epoch 10 Average Loss: 1.2777\n",
            "Epoch 11/100 Chunk 256/256 Loss: 1.8717 GPU: 0.0GB\n",
            "Epoch 11 Average Loss: 1.2358\n",
            "Epoch 12/100 Chunk 256/256 Loss: 1.8059 GPU: 0.0GB\n",
            "Epoch 12 Average Loss: 1.1955\n",
            "Epoch 13/100 Chunk 256/256 Loss: 1.7392 GPU: 0.0GB\n",
            "Epoch 13 Average Loss: 1.1529\n",
            "Epoch 14/100 Chunk 256/256 Loss: 1.6757 GPU: 0.0GB\n",
            "Epoch 14 Average Loss: 1.1043\n",
            "Epoch 15/100 Chunk 256/256 Loss: 1.6102 GPU: 0.0GB\n",
            "Epoch 15 Average Loss: 1.0516\n",
            "Epoch 16/100 Chunk 256/256 Loss: 1.5428 GPU: 0.0GB\n",
            "Epoch 16 Average Loss: 0.9918\n",
            "Epoch 17/100 Chunk 256/256 Loss: 1.4710 GPU: 0.0GB\n",
            "Epoch 17 Average Loss: 0.9249\n",
            "Epoch 18/100 Chunk 256/256 Loss: 1.3925 GPU: 0.0GB\n",
            "Epoch 18 Average Loss: 0.8527\n",
            "Epoch 19/100 Chunk 256/256 Loss: 1.3092 GPU: 0.0GB\n",
            "Epoch 19 Average Loss: 0.7777\n",
            "Epoch 20/100 Chunk 256/256 Loss: 1.2172 GPU: 0.0GB\n",
            "Epoch 20 Average Loss: 0.7005\n",
            "Epoch 21/100 Chunk 256/256 Loss: 1.1263 GPU: 0.0GB\n",
            "Epoch 21 Average Loss: 0.6229\n",
            "Epoch 22/100 Chunk 256/256 Loss: 1.0313 GPU: 0.0GB\n",
            "Epoch 22 Average Loss: 0.5467\n",
            "Epoch 23/100 Chunk 256/256 Loss: 0.9395 GPU: 0.0GB\n",
            "Epoch 23 Average Loss: 0.4698\n",
            "Epoch 24/100 Chunk 256/256 Loss: 0.8438 GPU: 0.0GB\n",
            "Epoch 24 Average Loss: 0.3972\n",
            "Epoch 25/100 Chunk 256/256 Loss: 0.7618 GPU: 0.0GB\n",
            "Epoch 25 Average Loss: 0.3255\n",
            "Epoch 26/100 Chunk 256/256 Loss: 0.6898 GPU: 0.0GB\n",
            "Epoch 26 Average Loss: 0.2634\n",
            "Epoch 27/100 Chunk 256/256 Loss: 0.6237 GPU: 0.0GB\n",
            "Epoch 27 Average Loss: 0.2041\n",
            "Epoch 28/100 Chunk 256/256 Loss: 0.5641 GPU: 0.0GB\n",
            "Epoch 28 Average Loss: 0.1524\n",
            "Epoch 29/100 Chunk 256/256 Loss: 0.5131 GPU: 0.0GB\n",
            "Epoch 29 Average Loss: 0.1055\n",
            "Epoch 30/100 Chunk 256/256 Loss: 0.4722 GPU: 0.0GB\n",
            "Epoch 30 Average Loss: 0.0638\n",
            "Epoch 31/100 Chunk 256/256 Loss: 0.4193 GPU: 0.0GB\n",
            "Epoch 31 Average Loss: 0.0364\n",
            "Epoch 32/100 Chunk 256/256 Loss: 0.3900 GPU: 0.0GB\n",
            "Epoch 32 Average Loss: 0.0098\n",
            "Epoch 33/100 Chunk 256/256 Loss: 0.3571 GPU: 0.0GB\n",
            "Epoch 33 Average Loss: -0.0138\n",
            "Epoch 34/100 Chunk 256/256 Loss: 0.3314 GPU: 0.0GB\n",
            "Epoch 34 Average Loss: -0.0228\n",
            "Epoch 35/100 Chunk 256/256 Loss: 0.3050 GPU: 0.0GB\n",
            "Epoch 35 Average Loss: -0.0250\n",
            "Epoch 36/100 Chunk 256/256 Loss: 0.2789 GPU: 0.0GB\n",
            "Epoch 36 Average Loss: -0.0051\n",
            "Epoch 37/100 Chunk 256/256 Loss: 0.2538 GPU: 0.0GB\n",
            "Epoch 37 Average Loss: 0.0386\n",
            "Epoch 38/100 Chunk 256/256 Loss: 0.2406 GPU: 0.0GB\n",
            "Epoch 38 Average Loss: 0.0350\n",
            "Epoch 39/100 Chunk 256/256 Loss: 0.2446 GPU: 0.0GB\n",
            "Epoch 39 Average Loss: -0.0111\n",
            "Epoch 40/100 Chunk 256/256 Loss: 0.2352 GPU: 0.0GB\n",
            "Epoch 40 Average Loss: -0.0269\n",
            "Epoch 41/100 Chunk 256/256 Loss: 0.2248 GPU: 0.0GB\n",
            "Epoch 41 Average Loss: -0.0359\n",
            "Epoch 42/100 Chunk 256/256 Loss: 0.2194 GPU: 0.0GB\n",
            "Epoch 42 Average Loss: -0.0490\n",
            "Epoch 43/100 Chunk 256/256 Loss: 0.2018 GPU: 0.0GB\n",
            "Epoch 43 Average Loss: -0.0562\n",
            "Epoch 44/100 Chunk 256/256 Loss: 0.1948 GPU: 0.0GB\n",
            "Epoch 44 Average Loss: -0.0760\n",
            "Epoch 45/100 Chunk 256/256 Loss: 0.1915 GPU: 0.0GB\n",
            "Epoch 45 Average Loss: -0.0873\n",
            "Epoch 46/100 Chunk 256/256 Loss: 0.1812 GPU: 0.0GB\n",
            "Epoch 46 Average Loss: -0.1046\n",
            "Epoch 47/100 Chunk 256/256 Loss: 0.1737 GPU: 0.0GB\n",
            "Epoch 47 Average Loss: -0.1206\n",
            "Epoch 48/100 Chunk 256/256 Loss: 0.1681 GPU: 0.0GB\n",
            "Epoch 48 Average Loss: -0.1400\n",
            "Epoch 49/100 Chunk 256/256 Loss: 0.1622 GPU: 0.0GB\n",
            "Epoch 49 Average Loss: -0.1585\n",
            "Epoch 50/100 Chunk 256/256 Loss: 0.1654 GPU: 0.0GB\n",
            "Epoch 50 Average Loss: -0.1789\n",
            "Epoch 51/100 Chunk 256/256 Loss: 0.1700 GPU: 0.0GB\n",
            "Epoch 51 Average Loss: -0.2018\n",
            "Epoch 52/100 Chunk 256/256 Loss: 0.1608 GPU: 0.0GB\n",
            "Epoch 52 Average Loss: -0.2150\n",
            "Epoch 53/100 Chunk 256/256 Loss: 0.1693 GPU: 0.0GB\n",
            "Epoch 53 Average Loss: -0.2273\n",
            "Epoch 54/100 Chunk 256/256 Loss: 0.1740 GPU: 0.0GB\n",
            "Epoch 54 Average Loss: -0.2434\n",
            "Epoch 55/100 Chunk 256/256 Loss: 0.1754 GPU: 0.0GB\n",
            "Epoch 55 Average Loss: -0.2514\n",
            "Epoch 56/100 Chunk 256/256 Loss: 0.1719 GPU: 0.0GB\n",
            "Epoch 56 Average Loss: -0.2645\n",
            "Epoch 57/100 Chunk 256/256 Loss: 0.1633 GPU: 0.0GB\n",
            "Epoch 57 Average Loss: -0.2754\n",
            "Epoch 58/100 Chunk 256/256 Loss: 0.1521 GPU: 0.0GB\n",
            "Epoch 58 Average Loss: -0.2911\n",
            "Epoch 59/100 Chunk 256/256 Loss: 0.1595 GPU: 0.0GB\n",
            "Epoch 59 Average Loss: -0.3005\n",
            "Epoch 60/100 Chunk 256/256 Loss: 0.1635 GPU: 0.0GB\n",
            "Epoch 60 Average Loss: -0.3083\n",
            "Epoch 61/100 Chunk 256/256 Loss: 0.1534 GPU: 0.0GB\n",
            "Epoch 61 Average Loss: -0.3210\n",
            "Epoch 62/100 Chunk 256/256 Loss: 0.1540 GPU: 0.0GB\n",
            "Epoch 62 Average Loss: -0.3303\n",
            "Epoch 63/100 Chunk 256/256 Loss: 0.1497 GPU: 0.0GB\n",
            "Epoch 63 Average Loss: -0.3388\n",
            "Epoch 64/100 Chunk 256/256 Loss: 0.1515 GPU: 0.0GB\n",
            "Epoch 64 Average Loss: -0.3519\n",
            "Epoch 65/100 Chunk 256/256 Loss: 0.1666 GPU: 0.0GB\n",
            "Epoch 65 Average Loss: -0.3726\n",
            "Epoch 66/100 Chunk 256/256 Loss: 0.1635 GPU: 0.0GB\n",
            "Epoch 66 Average Loss: -0.3817\n",
            "Epoch 67/100 Chunk 256/256 Loss: 0.1702 GPU: 0.0GB\n",
            "Epoch 67 Average Loss: -0.3896\n",
            "Epoch 68/100 Chunk 256/256 Loss: 0.1549 GPU: 0.0GB\n",
            "Epoch 68 Average Loss: -0.3963\n",
            "Epoch 69/100 Chunk 256/256 Loss: 0.1589 GPU: 0.0GB\n",
            "Epoch 69 Average Loss: -0.4063\n",
            "Epoch 70/100 Chunk 256/256 Loss: 0.1614 GPU: 0.0GB\n",
            "Epoch 70 Average Loss: -0.4260\n",
            "Epoch 71/100 Chunk 256/256 Loss: 0.1682 GPU: 0.0GB\n",
            "Epoch 71 Average Loss: -0.4391\n",
            "Epoch 72/100 Chunk 256/256 Loss: 0.1665 GPU: 0.0GB\n",
            "Epoch 72 Average Loss: -0.4405\n",
            "Epoch 73/100 Chunk 256/256 Loss: 0.1698 GPU: 0.0GB\n",
            "Epoch 73 Average Loss: -0.4518\n",
            "Epoch 74/100 Chunk 256/256 Loss: 0.1565 GPU: 0.0GB\n",
            "Epoch 74 Average Loss: -0.4576\n",
            "Epoch 75/100 Chunk 256/256 Loss: 0.1650 GPU: 0.0GB\n",
            "Epoch 75 Average Loss: -0.4701\n",
            "Epoch 76/100 Chunk 256/256 Loss: 0.1454 GPU: 0.0GB\n",
            "Epoch 76 Average Loss: -0.4854\n",
            "Epoch 77/100 Chunk 256/256 Loss: 0.1748 GPU: 0.0GB\n",
            "Epoch 77 Average Loss: -0.4947\n",
            "Epoch 78/100 Chunk 256/256 Loss: 0.1740 GPU: 0.0GB\n",
            "Epoch 78 Average Loss: -0.5140\n",
            "Epoch 79/100 Chunk 256/256 Loss: 0.1896 GPU: 0.0GB\n",
            "Epoch 79 Average Loss: -0.5243\n",
            "Epoch 80/100 Chunk 256/256 Loss: 0.2072 GPU: 0.0GB\n",
            "Epoch 80 Average Loss: -0.5309\n",
            "Epoch 81/100 Chunk 256/256 Loss: 0.1846 GPU: 0.0GB\n",
            "Epoch 81 Average Loss: -0.5226\n",
            "Epoch 82/100 Chunk 256/256 Loss: 0.1668 GPU: 0.0GB\n",
            "Epoch 82 Average Loss: -0.5225\n",
            "Epoch 83/100 Chunk 256/256 Loss: 0.1597 GPU: 0.0GB\n",
            "Epoch 83 Average Loss: -0.5326\n",
            "Epoch 84/100 Chunk 256/256 Loss: 0.1622 GPU: 0.0GB\n",
            "Epoch 84 Average Loss: -0.5473\n",
            "Epoch 85/100 Chunk 256/256 Loss: 0.1782 GPU: 0.0GB\n",
            "Epoch 85 Average Loss: -0.5670\n",
            "Epoch 86/100 Chunk 256/256 Loss: 0.1913 GPU: 0.0GB\n",
            "Epoch 86 Average Loss: -0.5814\n",
            "Epoch 87/100 Chunk 256/256 Loss: 0.2092 GPU: 0.0GB\n",
            "Epoch 87 Average Loss: -0.5898\n",
            "Epoch 88/100 Chunk 256/256 Loss: 0.2089 GPU: 0.0GB\n",
            "Epoch 88 Average Loss: -0.5973\n",
            "Epoch 89/100 Chunk 256/256 Loss: 0.2144 GPU: 0.0GB\n",
            "Epoch 89 Average Loss: -0.5912\n",
            "Epoch 90/100 Chunk 256/256 Loss: 0.1888 GPU: 0.0GB\n",
            "Epoch 90 Average Loss: -0.5751\n",
            "Epoch 91/100 Chunk 256/256 Loss: 0.1429 GPU: 0.0GB\n",
            "Epoch 91 Average Loss: -0.5575\n",
            "Epoch 92/100 Chunk 256/256 Loss: 0.1355 GPU: 0.0GB\n",
            "Epoch 92 Average Loss: -0.5792\n",
            "Epoch 93/100 Chunk 256/256 Loss: 0.1660 GPU: 0.0GB\n",
            "Epoch 93 Average Loss: -0.6032\n",
            "Epoch 94/100 Chunk 256/256 Loss: 0.1739 GPU: 0.0GB\n",
            "Epoch 94 Average Loss: -0.6350\n",
            "Epoch 95/100 Chunk 256/256 Loss: 0.2153 GPU: 0.0GB\n",
            "Epoch 95 Average Loss: -0.6545\n",
            "Epoch 96/100 Chunk 256/256 Loss: 0.2521 GPU: 0.0GB\n",
            "Epoch 96 Average Loss: -0.6678\n",
            "Epoch 97/100 Chunk 256/256 Loss: 0.3018 GPU: 0.0GB\n",
            "Epoch 97 Average Loss: -0.6835\n",
            "Epoch 98/100 Chunk 256/256 Loss: 0.3222 GPU: 0.0GB\n",
            "Epoch 98 Average Loss: -0.6834\n",
            "Epoch 99/100 Chunk 256/256 Loss: 0.3097 GPU: 0.0GB\n",
            "Epoch 99 Average Loss: -0.6624\n",
            "Epoch 100/100 Chunk 256/256 Loss: 0.2212 GPU: 0.0GB\n",
            "Epoch 100 Average Loss: -0.5897\n",
            "\n",
            "Generating predictions...\n",
            "\n",
            "Predictions:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-70-b551d43795d5>:16: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
            "  times = pd.date_range(start=pd.Timestamp.now(), periods=7, freq='10T')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Length of values (6) does not match length of index (7)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-bfa9afa768b5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nPredictions:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-70-b551d43795d5>\u001b[0m in \u001b[0;36mdisplay_predictions\u001b[0;34m(predictions, confidence)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdisplay_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimestamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiods\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'10T'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     return pd.DataFrame({\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;34m'Price'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;34m'Lower_CI'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.96\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mconfidence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_homogenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;31m# _homogenize ensures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m#  - all(len(x) == len(index) for x in arrays)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_homogenize\u001b[0;34m(data, index, dtype)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mrefs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \"\"\"\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    574\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length of values (6) does not match length of index (7)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics\n",
        "def evaluate_model(model, test_data, test_targets):\n",
        "    predictions, confidence = predict(model, test_data)\n",
        "\n",
        "    metrics = {\n",
        "        'RMSE': np.sqrt(((predictions - test_targets) ** 2).mean()),\n",
        "        'MAE': np.abs(predictions - test_targets).mean(),\n",
        "        'MAPE': np.abs((predictions - test_targets) / test_targets).mean() * 100,\n",
        "        'Direction': (np.sign(predictions[1:] - predictions[:-1]) ==\n",
        "                     np.sign(test_targets[1:] - test_targets[:-1])).mean()\n",
        "    }\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "KYqcHobAD5aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_results(model, test_data, test_targets):\n",
        "    preds, conf = predict(model, test_data)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(test_targets, label='Actual')\n",
        "    plt.plot(preds, label='Predicted')\n",
        "    plt.fill_between(range(len(preds)),\n",
        "                    preds - 1.96 * conf,\n",
        "                    preds + 1.96 * conf,\n",
        "                    alpha=0.3)\n",
        "    plt.title('Gold Price Predictions with 95% CI')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "wBnKW_EDD5VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y_ekyKkrD5P_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}